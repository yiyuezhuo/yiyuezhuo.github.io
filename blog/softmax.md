神经网络最后一层经常设定为softmax层，这是什么呢？

# Softmax

## logistic 回归

在机器学习式的logistic回归中，并不关心假设分布，只是在意预测准确度，
预测准确度的上升可以由损失函数的下降来表示。logistic回归本身预设了
一个函数形式(假设函数 hypothesis function)对各个特征进行融合以进行预测，
如下式所示

$$
h_\theta (x)=\frac{1}{1+exp(-\theta^Tx}
$$

之后，我们在这个模型假设上面作用一个优化算法，以试图找出该函数族预测效果
最好的那个，这个最好性就是直接以loss函数的最小性来定义的。所以我们的问题
就转化为了对参数进行优化使得loss函数最小化。

对于解析来说，直接求稳定点是一种方法，但是对于数值算法来说，梯度下降才是
合适的策略，于是我们对全体参数求导，如果损失函数是面向全体样本的，就是
直接梯度下降，如果是单独面向每个样本的，就是随机梯度下降。下面是直接
梯度下降的loss函数的一种定义方式

$$
J(\theta)=-\frac{1}{m}[\sum_{i=1}^my^{(i)}logh_\theta (x^{(i)})+(1-y^{(i)})log(1-h_\theta (x^{(i)}))]
$$

因为假设函数的输出一定在0,1之间，所以可以看出其在样本为正例(y=1)时
预测值趋于1可以使损失函数在这部分贡献趋于0，反之类似。它的确刻画了
我们想让假设函数输出类似训练集的输出的思想。

如此迭代若干轮后，我们得到一个参数集确定的假设函数。然后我们就可以给它
输出向量$x$来进行预测了。即这个$x$与参数向量$\theta$求点积后的东西
标准化变换到0,1区间上。$\theta$刻画了输入向量对应维/特征对于判定
它是正例还是反例的倾向性与强度进行刻画。

如我之前搞得对拿破仑各个战役的logistic回归中，如果不在意假设分布的话，
缪拉的虚拟变量对应的$\theta_i$就是负的，考虑到1表示的是法军胜利，所以
缪拉实际上对法军的战胜，相对于“平均水平”，是负的。而拿破仑，威灵顿这些
虚拟变量当然是把胜率往它们的阵营拉了。

## softmax回归

### 隶属多分类表达问题

softmax是logistic回归的推广，softmax回归预测的是隶属多个相斥的类的概率
如X是苹果香蕉还是梨。当softmax只处理两个类时，其数学形式与logistic相同。

人们不知道softmax回归的一个重要原因是一定程度上可以只使用logistic回归来处理它
处理的问题。比如可以进行三个分离的"是苹果/不是苹果，是香蕉不是香蕉,是
梨，不是梨"的logistic回归。这当然并不是等价的，容易看出它实际上丢失了一些互斥
的先验信息。

### logistic 的推广

考虑logstic回归的假设函数，我们可以这样理解它：函数赋予赋予是与否两个分类
以一个权值，再由权值标准化（除以两个权值的和）来表示分属两个类的概率。
我们一般看到的那个函数只是“是”那个类的概率。另一个可以直接以1-P(是)的
概率来予以表示。

可以看出，我们可以把其中一个类的权值设成固定的，因为变动一个权值就足以
确定确定关系了。实际上，我们把设"是"这个类权值固定为1，就像布尔代数
的传统一样。然后设"否"的权值为$exp(-\theta^Tx)$。标准化后的"是"的
概率预测恰好就是原来logistic回归的假设函数。

这种观点的好处在于可以直接推广出softmax的函数格式。softmax函数就是
把所有所有类的权重分量写出，也不使用固定为1这种技巧进行无谓简化的。
然后对向量进行标准化的假设函数。

### loss函数

$$
J(\theta)=-\frac{1}{m}[\sum_{i=1}^m\sum_{j=1}^k1(y^{(i)}=j)log\frac{e^{\theta_j^Tx(i)}}{\sum_{i=1}^k e^{e_l^Tx^{(i)}}}]
$$

具体来说就是每个样例会在且仅在其示性函数取1的那个地方，根据预测概率命中的对数
来提供loss，预测概率的对数表明预测越接近对loss贡献趋于0.

此函数可以自然地使用梯度下降法进行优化。

### 冗余参数问题

我们之前讨论logistic回归的推广时发现其softmax版本实际上引入了冗余的参数
因为实际上有一个权重固定就够了，实际上却引入了一套用于估计起码一个没用
的权重的系数的参数族。这就造成了冗余，一个现象就是任意一个单独的预测，
对其使用的各个系数向量减去一个相同的向量，得到的结果(各类隶属概率向量)
结果不变。

这种冗余并不会带来局部极小值问题，但是会带来数值问题。

我们实际上可以做完全类似于logistic中的“简化”，比如把一个权重始终设为1.
不过这对于对参数的分析有些含糊不清（不然为何要用logistic回归），另一种
改善方法是引入一个权重衰减项与原来的loss函数线性组合。

$$
\frac{\lambda}{2}\sum_{i=1}^k\sum_{j=0}^n\theta_{ij}^2
$$

由于某些原因，这就可以解决冗余参数带来的数值问题(hessen矩阵不可逆)。

## logsitic与softmax的随机模型

统计学上logistic回归对样本单个观测来源假设的分布是伯努利分布。最重要的假设就是
这个伯努利分布的p参数的值等于特征系数与特征的值线性组合的logit变换后
的结果。这时候这个回归模型的极大似然估计就相当于在调整特征系数来取
似然函数的最小值。以此确定系数，最后结果就是确定了样本的一个随机模型，
当然回归一般都是用其的条件期望函数，而这个随机模型的条件期望函数就是
机器学习里logsitic回归训练出的那个函数。

softmax完全类似，只不过从伯努利分布变成了按顺序排的离散分布。就求解
来说伯努利分布会产生二项分布，softmax产生那个分布叫多项分布。类似于
确定了多元线性回归里(为了求极大似然的系数)指定一个对应的整体随机模型。
然后在这个整体随机模型上比较原样本来求似然函数值。这里二项分布，多项分布
类似于前面多元正态分布的地位。它们都是(为了“搜索”)事先给定系数后先确定的。


## 作为的神经网络一层

softmax经常作为神经网络的最后一层，正如神经网络的最后一层(用做分类时)
应当输出一个隶属概率一样。这相当于在最后一步又做了一次softmax/logistic回归
，如此，神经网络之前可以看成是在利用类似softmax/logistic回归的方法在提取
适合最后一步softmax/logistic回归处理的特征，虽然前面一般也进行了0,1压缩，但
由于由很多线性组合而成这并不是真的。只有最后一步才能确保隶属概率向量性，
该性质将导致可以使用一些特别的loss函数，如categorical_crossentropy。

# Dropout

## 正则化问题

对于模型的泛化能力机器学习所提供的最强的先验信息当然是你先验地从可能模型族
中选一个特定形式作为假设函数进行拟合/学习。除此以外，我们还可以在假设函数
与计算方法导出的loss函数旁边符合另一个正则化函数/项来提升泛化能力。

究其具体，泛化能力一个很可能的来源是奥卡姆剃刀原则，为了量化奥卡姆剃刀原则，
我们可以说模型越简单越好，一方面这可以通过先验选择模型约束着手。但也可以在模型
内部优化内部进行表示，实际上我们可以说一个复杂模型里一些系数使得项消失，比如线性组合
的系数为0越多越好这种目标与原来的loss函数一同优化即可。

直接用为0系数数量为优化目标函数，或者说非0项个数为loss贡献的，为0范数。
取系数绝对值作为loss贡献的，为1范数。取二次方者，为二范数，我们之前见到的
权重衰减项就是这种正则化项。

正则化项也被用来表示对其他模型偏好的表示，由于原loss函数通常是假设函数与
诸如均方误差之类的计算方法的紧密复合。明显的解耦的方式也是作为线性组合放在一边。

## Dropout

dropout就是在训练是以一定概率使神经元/节点脱离前进与反传过程。这些做据说
可以消除一些网络自我过拟合问题，提升泛化能力，不过这样不会也影响提取某种更
深入的特征吗。

