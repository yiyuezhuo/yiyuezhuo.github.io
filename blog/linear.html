<!DOCTYPE html>
<!-- saved from url=(0059)file:///F:/yiyuezhuo.github.io/app/mini_markdown/index.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>linear</title>
  <link type="text/css" rel="stylesheet" href="../static/test.css">
  </head>

<body>
	<div id="renderField"><p>试想一个二维空间，矩阵可以对应一个二维变化。这就是说，对于二维空间上的每个向量，都有另一个向量与其对应。这种对应的游历是一个二维上的变换，受到一定线性限制下的变换族与R2矩阵一一对应。</p>
<p>二维线性变换与R2矩阵一一对应，我们再来考虑另一种更特殊的变换。这种变换是线性变换的子集，可以这样表示：它由两个向量配合两个值描述，任何向量要想被其变换，要先分解为两个向量的线性组合，然后乘对应的值，再加起来。线性代数的讨论表明，这与可对角化矩阵对应，向量称为特征向量，值称为特征值。</p>
<p>由于特征值与特征向量是可对角化矩阵与一类线性变化子集的一种表示，考虑这种表示的关系就显得有意思，尤其吸引人的是，特征值与特征向量具有一种值-向量组相对独立的性质，而可对角化矩阵单独取子矩阵似乎有点意义不明，从而使得特征向量与特征值对研究可对角化矩阵与其对应的线性变换成了一种有信息量，新鲜的视角。</p>
<p>为了使得特征值与特征向量这一价值更明显，我们选择更狭隘的一种变换，这里特征向量之间必须是正交的，在二维情况里，就是垂直。我们发现，只要我们知道特征向量是来自一个互相正交的向量组，就可以实际不管向量组的其他向量的具体值与特征值而得到待变换向量在本特征向量上的坐标值，这是个重要的性质。试想我们知道一个二维变换的一个特征向量，现在指定一个待变换向量，则随着另一个特征向量取的不同，本特征向量上的坐标值也会变化，这非常糟糕。而现在这一类变换中，即使其他特征向量改变，虽然确定的变换不同，但在本特征向量这一方面，比如坐标值上却不会发生任何改变。这对于我们所追求的特征向量之间的某种独立性与不变性来说是极好的。</p>
<p>这种独立性的一个应用是，假如一个变换是通过这样的正交特征向量定义的，而一些特征向量的特征值远远大于另一些。则我们为了近似可以忽略掉那些特征值较小的特征向量，只使用那些特征值较大的近似表示原来那个变换。值得注意的是为什么我们可以这样做——本来这种变换是要先分解成n个坐标的，诚然我们可以先把向量分解成n个坐标，再忽略掉我们决定忽略的那些特征向量对应的坐标，比如设成0，进行变换。但这个分解是与所有特征向量相关的，我们实际上要解一个由所有特征向量决定的线性方程组做到这一点。而现在，我们实际上可以单凭单个特征向量本身获得其对应的坐标，不再需要解那个方程组，我们可以真正忽视掉那些冗余的特征向量。</p>
<p>反过来，由于一个这样的变换总对应一个对称可逆矩阵，我们实际上可以把一个这样的矩阵简化为几个正交向量与对应的特征值。我们看看如何在这种变换意义上从一个具体的对称可逆矩阵上做到这一点。事实上，我们先按标准方法求出特征向量组，然后使用施密特正交化过程得到一个标准正交基即可，它们对应的特征值可以代进去探测出来。此时是保持了完整信息而且做好准备的，这时可以丢弃一些特征向量/值对进行压缩。</p>
<p>这种压缩后的变换的使用方式是，给定待变换向量，求该向量与各剩余基的投影（标准化后直接等于内积）即为其上的坐标，乘特征值后再反过来线性组合各对即可。比较保持所有信息的版本，即与原来直接矩阵相乘等价的保持所有对的线性组合。可以发现如果特征值非常小的话，对变换结果的影响就微乎其微，可以忽略，从而验证了上面的讨论。</p>
<p>如果我们想从变换中还原出一个n*n矩阵继续使用矩阵相乘的表示，或者就是想压缩原来那个矩阵，一个显然的想法就是考虑保持所有信息的特征向量特征值组是如何还原的。这正好就是这一类正交对角化的讨论内容。如果从中去掉一些特征向量，则可以验证去掉这些向量其实并不会改变矩阵的维数。所以利用对角化那个形式就可以（有损）还原原矩阵（具体来说，中间放部分特征值矩阵，两边以恰当的顺序放部分特征向量矩阵及其转置）。</p>
<p>这种压缩只能用在对称可逆矩阵上，似乎比较受限。然而实际上经常碰到对称可逆矩阵，比如简单加权图和连接矩阵与随机变量的协方差矩阵。其中简单加权图虽然经常是可逆的，但并不是必然的。而协方差矩阵却是完美符合要求的对象。下面以协方差矩阵的PCA（主成分分析）为例讨论上面提到的性质。</p>
<p>给定m*n的数据矩阵，即有m个“观测”，n个变量/特征的数据。我们想要压缩特征，可以研究它们的互相表出关系，如果其中一部分特别容易被其他表出，就可以把它们剔除掉，用线性表出式来占位。这是从近似完全信息的角度来看，如果从另一个角度看，一个容易被表出的变量不够独立，没有很大的价值，直接剔除连表达式都不留下也无妨。为了做到这一点，可以研究上述数据矩阵对应的样本协方差矩阵，处于处理的便利起见，我们先进行标准化，即减去均值再除以其标准差。</p>
<p>对于这个协方差矩阵，可以计算出各正交特征向量组。其中特征值最大的那些引起了我们的注意，可是它们意义何在呢？从二维入手，我们将其降到一维，我们在做什么？</p>
<p>线性代数告诉我们，正交特征向量组中特征值最大的那个，所有数据点与其的误差最小化，这有点像一元线性回归的拟合直线，那个标准化向量射出去就类似这个线（但不相同）。同样类似一元线性回归的R2，这时我们可以谈论方程承载了多少方差/误差，承载了越多则拟合越好，这里则是说表示效果越好，这是PCA的最小平方误差解释，当然也有对偶的方差解释。对于这一点，考虑假如数据真是线性的，则放一个向量其下界总是0，并没有什么用，其上界即最大方差的线，则又是线性回归/PCA对应的那个线。对于多元情况，可以考虑使PCA变换后的拟合数据与原数据平方误差最小化的那个线/超平面。不过线性回归算的是单个预测变量y的误差平方，而PCA则是所有变量PCA变换（正交投影）到PCA超平面上与该点原来位置的欧几里得距离平方和最小化。PCA超平面正是正交特征向量支撑起来的。在这里线性代数的正交理论同时解决了最小二乘法和PCA超平面法的最小化问题。</p>
</div>
</body>
</html>